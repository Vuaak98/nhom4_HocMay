import os
import json
import re
import logging
import sys
from typing import Dict, List, Tuple, Any, Optional
import numpy as np
import pandas as pd
from tqdm import tqdm
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
from datetime import datetime

# ThÃªm thÆ° má»¥c gá»‘c vÃ o sys.path Ä‘á»ƒ import tá»« cÃ¡c module khÃ¡c trong src
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)

# Import cÃ¡c biáº¿n Ä‘Æ°á»ng dáº«n
from config import (
    DATA_DIR, RULES_PATH, STOPWORDS_PATH,
    DATA_PROCESSED_DIR, TRAIN_CLEANED_FILE, TEST_CLEANED_FILE,
    RULE_SYSTEM_MODEL_PATH, TRAIN_CONFUSION_MATRIX_PATH, TEST_CONFUSION_MATRIX_PATH,
    TRAIN_FEATURES_FILE, TEST_FEATURES_FILE,
    DATA_VISUALIZATION_DIR
)

# ÄÆ°á»ng dáº«n cho file JSON káº¿t quáº£
TRAIN_RESULTS_JSON_PATH = os.path.join(DATA_VISUALIZATION_DIR, 'train_results.json')
TEST_RESULTS_JSON_PATH = os.path.join(DATA_VISUALIZATION_DIR, 'test_results.json')

# === Cáº¥u hÃ¬nh logging ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger('main')

class RuleSystem:
    """Lá»›p há»‡ thá»‘ng luáº­t kÃ©p: lá»c tin vÃ  trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng."""
    
    def __init__(self, rules_path: str):
        """
        Khá»Ÿi táº¡o há»‡ thá»‘ng luáº­t.
        
        Args:
            rules_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file JSON chá»©a cÃ¡c luáº­t
        """
        self.rules_path = rules_path
        self.rules = self._load_rules()
        self.components = self.rules.get('pattern_components', {})
        self.percentiles = {}
        self.is_fitted = False
        logger.info(f"âœ… ÄÃ£ táº£i {len(self.components)} thÃ nh pháº§n luáº­t tá»« {rules_path}")
        logger.info("âœ… ÄÃ£ khá»Ÿi táº¡o RuleSystem")
    
    def _load_rules(self) -> Dict:
        """Táº£i cÃ¡c luáº­t tá»« file JSON."""
        try:
            with open(self.rules_path, 'r', encoding='utf-8') as f:
                rules = json.load(f)
            return rules
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi táº£i luáº­t tá»« {self.rules_path}: {str(e)}")
            raise
    
    def _is_match(self, text: str, component_name: str) -> bool:
        """Kiá»ƒm tra vÄƒn báº£n cÃ³ khá»›p vá»›i má»™t component khÃ´ng."""
        keywords = self.components.get(component_name, {}).get('keywords', [])
        return any(kw.lower() in text.lower() for kw in keywords)
    
    def _is_reliable_real_pattern(self, text: str) -> bool:
        """
        Kiá»ƒm tra xem vÄƒn báº£n cÃ³ khá»›p vá»›i máº«u tin tá»©c bÃ¡o chÃ­ khÃ´ng.
        
        Args:
            text: VÄƒn báº£n cáº§n kiá»ƒm tra
            
        Returns:
            bool: True náº¿u khá»›p vá»›i máº«u tin tá»©c bÃ¡o chÃ­
        """
        real_pattern = self.rules['patterns_for_filtering']['reliable_real_pattern']
        
        # Kiá»ƒm tra cÃ¡c Ä‘iá»u kiá»‡n báº¯t buá»™c
        must_have = real_pattern['must_have']
        must_not_have = real_pattern['must_not_have']
        
        # Kiá»ƒm tra tá»«ng Ä‘iá»u kiá»‡n báº¯t buá»™c
        has_authoritative = self._is_match(text, 'authoritative_source')
        has_informative = self._is_match(text, 'informative_tone')
        
        # Kiá»ƒm tra cÃ¡c Ä‘iá»u kiá»‡n cáº¥m
        has_pseudoscience = self._is_match(text, 'pseudoscience_hoax')
        has_scam = self._is_match(text, 'scam_call_to_action')
        has_spam = self._is_match(text, 'advertisement_spam')
        has_critical = self._is_match(text, 'critical_tone')
        
        # Log chi tiáº¿t cho debug
        logger.debug(f"\nKiá»ƒm tra máº«u tin tháº­t:")
        logger.debug(f"  - CÃ³ nguá»“n tin Ä‘Ã¡ng tin cáº­y: {has_authoritative}")
        logger.debug(f"  - CÃ³ vÄƒn phong bÃ¡o chÃ­: {has_informative}")
        logger.debug(f"  - CÃ³ dáº¥u hiá»‡u giáº£ khoa há»c: {has_pseudoscience}")
        logger.debug(f"  - CÃ³ dáº¥u hiá»‡u lá»«a Ä‘áº£o: {has_scam}")
        logger.debug(f"  - CÃ³ dáº¥u hiá»‡u spam: {has_spam}")
        logger.debug(f"  - CÃ³ dáº¥u hiá»‡u chá»‰ trÃ­ch: {has_critical}")
        
        # Náº¿u cÃ³ báº¥t ká»³ dáº¥u hiá»‡u xáº¥u nÃ o, loáº¡i bá» ngay
        if any([has_pseudoscience, has_scam, has_spam, has_critical]):
            return False
        
        # Pháº£i cÃ³ Cáº¢ nguá»“n tin Ä‘Ã¡ng tin cáº­y VÃ€ vÄƒn phong bÃ¡o chÃ­
        if not (has_authoritative and has_informative):
            return False
        
        return True
    
    def fit(self, df: pd.DataFrame) -> 'RuleSystem':
        """
        'Huáº¥n luyá»‡n' bá»™ luáº­t báº±ng cÃ¡ch tÃ­nh toÃ¡n cÃ¡c giÃ¡ trá»‹ thá»‘ng kÃª cáº§n thiáº¿t tá»« dá»¯ liá»‡u train.
        
        Args:
            df: DataFrame chá»©a dá»¯ liá»‡u train
            
        Returns:
            RuleSystem: Äá»‘i tÆ°á»£ng RuleSystem Ä‘Ã£ Ä‘Æ°á»£c fit
        """
        logger.info("ğŸ”„ Äang tÃ­nh toÃ¡n cÃ¡c giÃ¡ trá»‹ thá»‘ng kÃª tá»« dá»¯ liá»‡u train...")
        
        # TÃ­nh toÃ¡n cÃ¡c percentiles cho cÃ¡c cá»™t tÆ°Æ¡ng tÃ¡c
        interaction_cols = ['num_like_post', 'num_comment_post', 'num_share_post']
        for col in interaction_cols:
            if col in df.columns:
                self.percentiles[col] = {
                    'p25': df[col].quantile(0.25),
                    'p50': df[col].quantile(0.50),
                    'p75': df[col].quantile(0.75),
                    'p90': df[col].quantile(0.90),
                    'p95': df[col].quantile(0.95),
                    'p99': df[col].quantile(0.99)
                }
        
        # TÃ­nh toÃ¡n cÃ¡c ngÆ°á»¡ng cho cÃ¡c Ä‘áº·c trÆ°ng khÃ¡c
        if 'cleaned_message' in df.columns:
            # Tá»· lá»‡ chá»¯ hoa
            df['uppercase_ratio'] = df['cleaned_message'].str.findall(r'[A-Z]').str.len() / (df['cleaned_message'].str.len() + 1e-6)
            self.percentiles['uppercase_ratio'] = {
                'p75': df['uppercase_ratio'].quantile(0.75),
                'p90': df['uppercase_ratio'].quantile(0.90),
                'p95': df['uppercase_ratio'].quantile(0.95)
            }
            
            # Sá»‘ lÆ°á»£ng hashtag
            df['hashtag_count'] = df['cleaned_message'].str.count('#')
            self.percentiles['hashtag_count'] = {
                'p75': df['hashtag_count'].quantile(0.75),
                'p90': df['hashtag_count'].quantile(0.90),
                'p95': df['hashtag_count'].quantile(0.95)
            }
        
        self.is_fitted = True
        logger.info("âœ… ÄÃ£ hoÃ n thÃ nh viá»‡c tÃ­nh toÃ¡n cÃ¡c giÃ¡ trá»‹ thá»‘ng kÃª")
        return self
    
    def save(self, filepath: str):
        """
        LÆ°u toÃ n bá»™ Ä‘á»‘i tÆ°á»£ng RuleSystem Ä‘Ã£ Ä‘Æ°á»£c fit.
        
        Args:
            filepath: ÄÆ°á»ng dáº«n Ä‘á»ƒ lÆ°u file
        """
        if not self.is_fitted:
            logger.warning("âš ï¸ RuleSystem chÆ°a Ä‘Æ°á»£c fit, cÃ¡c giÃ¡ trá»‹ thá»‘ng kÃª cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c")
        
        try:
            joblib.dump(self, filepath)
            logger.info(f"âœ… ÄÃ£ lÆ°u Ä‘á»‘i tÆ°á»£ng RuleSystem vÃ o {filepath}")
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi lÆ°u RuleSystem: {str(e)}")
            raise
    
    @classmethod
    def load(cls, filepath: str) -> 'RuleSystem':
        """
        Táº£i má»™t Ä‘á»‘i tÆ°á»£ng RuleSystem Ä‘Ã£ Ä‘Æ°á»£c lÆ°u.
        
        Args:
            filepath: ÄÆ°á»ng dáº«n Ä‘áº¿n file Ä‘Ã£ lÆ°u
            
        Returns:
            RuleSystem: Äá»‘i tÆ°á»£ng RuleSystem Ä‘Ã£ Ä‘Æ°á»£c táº£i
        """
        try:
            model = joblib.load(filepath)
            logger.info(f"âœ… ÄÃ£ táº£i Ä‘á»‘i tÆ°á»£ng RuleSystem tá»« {filepath}")
            return model
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi táº£i RuleSystem: {str(e)}")
            raise
    
    def classify_difficulty(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        PhÃ¢n loáº¡i Ä‘á»™ khÃ³ cá»§a tin tá»©c.
        
        Args:
            df: DataFrame chá»©a dá»¯ liá»‡u
            
        Returns:
            pd.DataFrame: DataFrame Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n loáº¡i
        """
        def get_verdict(row):
            # Sá»­ dá»¥ng post_message gá»‘c, khÃ´ng dÃ¹ng cleaned_message
            text = str(row['post_message']).lower()
            
            # Kiá»ƒm tra cÃ¡c Ä‘iá»u kiá»‡n cho tin tháº­t dá»…
            is_reliable = self._is_reliable_real_pattern(text)
            
            # Kiá»ƒm tra cÃ¡c Ä‘iá»u kiá»‡n cho tin khÃ³
            has_pseudoscience = self._is_match(text, 'pseudoscience_hoax')
            has_scam = self._is_match(text, 'scam_call_to_action')
            has_spam = self._is_match(text, 'advertisement_spam')
            has_critical = self._is_match(text, 'critical_tone')
            
            # PhÃ¢n loáº¡i
            if is_reliable and not (has_pseudoscience or has_scam or has_spam or has_critical):
                return 'Tin Tháº­t Dá»…'
            else:
                return 'Tin KhÃ³'
        
        df_copy = df.copy()
        df_copy['case_difficulty'] = df_copy.apply(get_verdict, axis=1)
        
        # Log thá»‘ng kÃª phÃ¢n loáº¡i
        total = len(df_copy)
        easy_count = (df_copy['case_difficulty'] == 'Tin Tháº­t Dá»…').sum()
        hard_count = (df_copy['case_difficulty'] == 'Tin KhÃ³').sum()
        
        logger.info(f"\nThá»‘ng kÃª phÃ¢n loáº¡i:")
        logger.info(f"  - Tá»•ng sá»‘ máº«u: {total}")
        logger.info(f"  - Tin Tháº­t Dá»…: {easy_count} ({easy_count/total*100:.1f}%)")
        logger.info(f"  - Tin KhÃ³: {hard_count} ({hard_count/total*100:.1f}%)")
        
        return df_copy

    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Táº¡o cÃ¡c cá»™t Ä‘áº·c trÆ°ng sá»‘ tá»« cÃ¡c luáº­t cho mÃ´ hÃ¬nh ML.
        
        Args:
            df: DataFrame chá»©a dá»¯ liá»‡u
            
        Returns:
            pd.DataFrame: DataFrame vá»›i cÃ¡c Ä‘áº·c trÆ°ng má»›i
        """
        if not self.is_fitted:
            logger.warning("âš ï¸ RuleSystem chÆ°a Ä‘Æ°á»£c fit, cÃ¡c Ä‘áº·c trÆ°ng cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c")
        
        df_featured = df.copy()
        # Sá»­ dá»¥ng cleaned_message náº¿u cÃ³, ngÆ°á»£c láº¡i dÃ¹ng post_message
        df_featured['text_for_analysis'] = df_featured.get('cleaned_message', df_featured['post_message'])
        # Chuyá»ƒn Ä‘á»•i táº¥t cáº£ giÃ¡ trá»‹ sang string vÃ  chuyá»ƒn vá» chá»¯ thÆ°á»ng
        df_featured['text_for_analysis'] = df_featured['text_for_analysis'].astype(str)
        df_featured['text_for_analysis_lower'] = df_featured['text_for_analysis'].str.lower()
        
        # 1. TrÃ­ch xuáº¥t Ä‘iá»ƒm tá»« cÃ¡c component ná»™i dung
        for comp_name, details in self.components.items():
            feature_name = f'rule_score_{comp_name}'
            keywords = details.get('keywords', [])
            weight = details.get('weight', 0)
            
            df_featured[feature_name] = df_featured['text_for_analysis_lower'].apply(
                lambda text: sum(1 for kw in keywords if kw.lower() in str(text).lower()) * weight
            )
        
        # 2. Xá»­ lÃ½ cÃ¡c Ä‘áº·c trÆ°ng cháº¥t lÆ°á»£ng vÄƒn báº£n
        df_featured['feat_uppercase_ratio'] = df_featured['text_for_analysis'].str.findall(r'[A-Z]').str.len() / (df_featured['text_for_analysis'].str.len() + 1e-6)
        df_featured['feat_hashtag_count'] = df_featured['text_for_analysis'].str.count('#')
        df_featured['feat_url_count'] = df_featured['text_for_analysis'].str.count('http|www|<URL>')

        # 3. TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng metadata (tÆ°Æ¡ng tÃ¡c)
        meta_rules = self.rules.get('metadata_rules', {})
        
        # Tá»· lá»‡ share/like
        share_rule = meta_rules.get('high_share_ratio', {})
        ratio_threshold = share_rule.get('ratio_threshold', 2.0)
        min_likes = share_rule.get('min_likes', 50)
        weight = share_rule.get('weight', 2.0)
        
        df_featured['rule_score_high_share_ratio'] = (
            ((df_featured['num_share_post'] / (df_featured['num_like_post'] + 1)) > ratio_threshold) &
            (df_featured['num_like_post'] > min_likes)
        ).astype(int) * weight

        # Sá»‘ lÆ°á»£ng hashtag
        hashtag_rule = meta_rules.get('many_hashtags', {})
        hashtag_threshold = hashtag_rule.get('hashtag_threshold', 5)
        weight = hashtag_rule.get('weight', 1.0)
        df_featured['rule_score_many_hashtags'] = (df_featured['feat_hashtag_count'] > hashtag_threshold).astype(int) * weight

        # 4. TÃ­nh toÃ¡n tá»•ng Ä‘iá»ƒm fake vÃ  real
        fake_scores = [col for col in df_featured.columns if col.startswith('rule_score_') and col != 'rule_score_authoritative_source']
        real_scores = ['rule_score_authoritative_source']
        
        df_featured['feat_total_fake_score'] = df_featured[fake_scores].sum(axis=1)
        df_featured['feat_total_real_score'] = df_featured[real_scores].sum(axis=1)
        
        # 5. TÃ­nh toÃ¡n tá»· lá»‡ tin tháº­t
        df_featured['feat_truth_ratio'] = df_featured['feat_total_real_score'] / (df_featured['feat_total_fake_score'] + df_featured['feat_total_real_score'] + 1e-6)
        
        # 6. Kiá»ƒm tra xung Ä‘á»™t
        df_featured['feat_has_conflict'] = (
            (df_featured['feat_total_fake_score'] > 0) & 
            (df_featured['feat_total_real_score'] > 0)
        ).astype(int)
        
        # XÃ³a cÃ¡c cá»™t táº¡m thá»i
        df_featured = df_featured.drop(['text_for_analysis', 'text_for_analysis_lower'], axis=1)
        
        return df_featured

def analyze_and_save_results(df_classified: pd.DataFrame, dataset_name: str, output_path: str, cm_path: str):
    """
    PhÃ¢n tÃ­ch káº¿t quáº£ cá»§a Bá»˜ Lá»ŒC trÃªn cÃ¡c tin Ä‘Æ°á»£c phÃ¢n loáº¡i lÃ  Dá»„.
    """
    logger.info(f"\n=== PhÃ¢n tÃ­ch Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p {dataset_name} (chá»‰ xÃ©t cÃ¡c tin Dá»„) ===")

    # BÆ¯á»šC QUAN TRá»ŒNG: Lá»c ra cÃ¡c tin Ä‘Ã£ Ä‘Æ°á»£c bá»™ lá»c xá»­ lÃ½
    df_easy = df_classified[df_classified['case_difficulty'] != 'Tin KhÃ³'].copy()
    
    if df_easy.empty:
        logger.warning(f"KhÃ´ng cÃ³ 'Tin Dá»…' nÃ o Ä‘Æ°á»£c tÃ¬m tháº¥y trong táº­p {dataset_name}. Bá» qua phÃ¢n tÃ­ch.")
        return

    # Táº¡o nhÃ£n dá»± Ä‘oÃ¡n tá»« káº¿t quáº£ cá»§a bá»™ lá»c
    # 0 lÃ  Tháº­t, 1 lÃ  Giáº£
    df_easy['predicted_label'] = df_easy['case_difficulty'].apply(lambda x: 0 if x == 'Tin Tháº­t Dá»…' else 1)

    y_true = df_easy['label']
    y_pred = df_easy['predicted_label']

    # --- BÃ¡o cÃ¡o ra Console ---
    report_dict = classification_report(y_true, y_pred, target_names=['Tin Tháº­t (0)', 'Tin Giáº£ (1)'], output_dict=True, zero_division=0)
    logger.info(f"\nBÃ¡o cÃ¡o phÃ¢n loáº¡i trÃªn cÃ¡c tin Dá»„ cá»§a táº­p {dataset_name}:\n" +
                classification_report(y_true, y_pred, target_names=['Tin Tháº­t (0)', 'Tin Giáº£ (1)'], zero_division=0))

    # --- Váº½ vÃ  lÆ°u Ma tráº­n nháº§m láº«n ---
    cm = confusion_matrix(y_true, y_pred, labels=[0, 1]) # Chá»‰ Ä‘á»‹nh labels Ä‘á»ƒ Ä‘áº£m báº£o thá»© tá»± Ä‘Ãºng
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Dá»± Ä‘oÃ¡n Tháº­t', 'Dá»± Ä‘oÃ¡n Giáº£'], 
                yticklabels=['Thá»±c táº¿ Tháº­t', 'Thá»±c táº¿ Giáº£'])
    plt.title(f'Ma tráº­n nháº§m láº«n bá»™ lá»c trÃªn táº­p {dataset_name} (Tin Dá»…)')
    plt.ylabel('NhÃ£n Thá»±c táº¿')
    plt.xlabel('NhÃ£n Dá»± Ä‘oÃ¡n')
    plt.savefig(cm_path, bbox_inches='tight')
    plt.close()
    logger.info(f"âœ… ÄÃ£ lÆ°u ma tráº­n nháº§m láº«n vÃ o: {cm_path}")

    # --- LÆ°u káº¿t quáº£ chi tiáº¿t ra file JSON ---
    results = {
        'dataset_name': dataset_name,
        'total_samples_in_set': len(df_classified),
        'easy_samples_count': len(df_easy),
        'hard_samples_count': len(df_classified) - len(df_easy),
        'easy_samples_ratio': f"{len(df_easy) / len(df_classified) * 100:.2f}%" if len(df_classified) > 0 else "0.00%",
        'classification_report_on_easy_cases': report_dict,
        'confusion_matrix_on_easy_cases': cm.tolist()
    }
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=4, ensure_ascii=False)
    logger.info(f"âœ… ÄÃ£ lÆ°u bÃ¡o cÃ¡o chi tiáº¿t vÃ o: {output_path}")

def main():
    """HÃ m chÃ­nh Ä‘á»ƒ cháº¡y toÃ n bá»™ quy trÃ¬nh."""
    try:
        # Kiá»ƒm tra xem cÃ¡c file Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch Ä‘Ã£ tá»“n táº¡i chÆ°a
        if not os.path.exists(TRAIN_CLEANED_FILE) or not os.path.exists(TEST_CLEANED_FILE):
            logger.error("âŒ KhÃ´ng tÃ¬m tháº¥y cÃ¡c file dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch.")
            logger.error("HÃ£y cháº¡y 'src/data_processing.py' trÆ°á»›c Ä‘á»ƒ táº¡o cÃ¡c file nÃ y.")
            return
            
        # Kiá»ƒm tra xem cÃ¡c file cÃ³ trá»‘ng khÃ´ng
        if os.path.getsize(TRAIN_CLEANED_FILE) == 0 or os.path.getsize(TEST_CLEANED_FILE) == 0:
            logger.error("âŒ CÃ¡c file dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch Ä‘ang trá»‘ng.")
            logger.error("HÃ£y cháº¡y láº¡i 'src/data_processing.py' Ä‘á»ƒ táº¡o dá»¯ liá»‡u.")
            return
            
        # 1. Táº£i dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch tá»« data_processing.py
        logger.info("ğŸ”„ Äang táº£i dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch tá»« data_processing.py...")
        
        # Chá»‰ Ä‘á»‹nh kiá»ƒu dá»¯ liá»‡u khi Ä‘á»c CSV
        dtype_dict = {
            'post_message': str,
            'cleaned_message': str,
            'text_length': 'Int64',
            'word_count': 'Int64',
            'sentence_count': 'Int64',
            'hashtag_count': 'Int64',
            'url_count': 'Int64',
            'stopwords_ratio': float,
            'compound_word_ratio': float,
            'num_like_post': 'Int64',
            'num_comment_post': 'Int64',
            'num_share_post': 'Int64'
        }
        
        train_df = pd.read_csv(TRAIN_CLEANED_FILE, dtype=dtype_dict)
        test_df = pd.read_csv(TEST_CLEANED_FILE, dtype=dtype_dict)
        
        # Kiá»ƒm tra xem dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½ chÆ°a
        required_columns = [
            'cleaned_message', 'text_length', 'word_count', 'sentence_count',
            'hashtag_count', 'url_count', 'stopwords_ratio', 'compound_word_ratio'
        ]
        
        missing_columns = [col for col in required_columns if col not in train_df.columns]
        if missing_columns:
            logger.error(f"âŒ Thiáº¿u cÃ¡c cá»™t quan trá»ng trong dá»¯ liá»‡u: {missing_columns}")
            logger.error("HÃ£y cháº¡y láº¡i 'src/data_processing.py' Ä‘á»ƒ táº¡o Ä‘áº§y Ä‘á»§ cÃ¡c cá»™t.")
            return
            
        logger.info(f"âœ… ÄÃ£ táº£i {len(train_df)} máº«u train vÃ  {len(test_df)} máº«u test")
        
        # 2. Khá»Ÿi táº¡o vÃ  huáº¥n luyá»‡n há»‡ thá»‘ng luáº­t
        logger.info("ğŸ”„ Äang khá»Ÿi táº¡o vÃ  huáº¥n luyá»‡n há»‡ thá»‘ng luáº­t...")
        rule_system = RuleSystem(RULES_PATH)
        rule_system.fit(train_df)
        
        # 3. PhÃ¢n loáº¡i Ä‘á»™ khÃ³ vÃ  trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng
        logger.info("ğŸ”„ Äang phÃ¢n loáº¡i Ä‘á»™ khÃ³ vÃ  trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng...")
        train_classified = rule_system.classify_difficulty(train_df)
        test_classified = rule_system.classify_difficulty(test_df)
        
        # 4. TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng vÃ  lÆ°u káº¿t quáº£
        logger.info("ğŸ”„ Äang trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng...")
        train_features = rule_system.extract_features(train_classified)
        test_features = rule_system.extract_features(test_classified)
        
        # LÆ°u cÃ¡c Ä‘áº·c trÆ°ng
        train_features.to_csv(TRAIN_FEATURES_FILE, index=False)
        test_features.to_csv(TEST_FEATURES_FILE, index=False)
        logger.info(f"âœ… ÄÃ£ lÆ°u cÃ¡c Ä‘áº·c trÆ°ng vÃ o {TRAIN_FEATURES_FILE} vÃ  {TEST_FEATURES_FILE}")
        
        # 5. PhÃ¢n tÃ­ch vÃ  lÆ°u káº¿t quáº£
        logger.info("ğŸ”„ Äang phÃ¢n tÃ­ch káº¿t quáº£...")
        analyze_and_save_results(train_classified, 'train', TRAIN_RESULTS_JSON_PATH, TRAIN_CONFUSION_MATRIX_PATH)
        analyze_and_save_results(test_classified, 'test', TEST_RESULTS_JSON_PATH, TEST_CONFUSION_MATRIX_PATH)
        
        # 6. LÆ°u mÃ´ hÃ¬nh
        rule_system.save(RULE_SYSTEM_MODEL_PATH)
        logger.info(f"âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o {RULE_SYSTEM_MODEL_PATH}")
        
        logger.info("âœ… ÄÃ£ hoÃ n thÃ nh toÃ n bá»™ quy trÃ¬nh!")
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i trong quy trÃ¬nh: {str(e)}")
        raise

if __name__ == "__main__":
    main() 